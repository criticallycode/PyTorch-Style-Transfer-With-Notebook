{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates an example of neural style transfer in Pytorch. Neural style transfer is where the style of the pixels that make up one image is transferred to another image. The content of the second image is preserved, it is just rendered in a style that mimics the style of the first image. \n",
    "\n",
    "Unlike in an image classification or recognition task, two loss metrics are used instead of just one. A content loss will be determined as well as a style loss, and the two loss metrics will be combined to deliver a representation between the total loss for the content and the total loss for the style."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start out with, we want to import all the libraries and models we need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to define a function to load in the images and transform them into tensors that the model can work with. Before we do that, we're going to need to specify the size of the image we want to work with. \n",
    "\n",
    "After we define these variables, we'll create the list of transforms we want to use. We're going to create a function to load in the image using our transforms. We'll use the transforms we specified earlier as well as the Image function from PIL to import the images and turn them into tensors. \n",
    "\n",
    "We also need to normalize the channels within the model and send the normalized data to the device. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function loads image and transforms to PyTorch Tensor, also normalizes\n",
    "# specify a max size and an optional shape\n",
    "# normalization numbers come from recommended numbers used on the ImageNet dataset\n",
    "\n",
    "def image_load(img_path, max_size=800, shape=None):\n",
    "\n",
    "    # open the image and convert it to RGB\n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "    # if the total image size is greater than our specified size,\n",
    "    # recast to chosen max size\n",
    "    if max(image.size) > max_size:\n",
    "        size = max_size\n",
    "    else:\n",
    "        size = max(image.size)\n",
    "\n",
    "\n",
    "    if shape is not None:\n",
    "        size = shape\n",
    "\n",
    "\n",
    "    im_transforms = transforms.Compose([transforms.Resize((size, int(1.5* size))),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "                                        ])\n",
    "\n",
    "    # unsqueeze the image after doing transforms\n",
    "    # unsqueeze takes input, dimensions and output tensor(optional)\n",
    "    # this discards the transparent alpha channel (:3) and adds batch dimension\n",
    "    image = im_transforms(image)[:3, :, :].unsqueeze(0)\n",
    "\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's be sure that the images are loading in as we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 800, 1200])\n"
     ]
    }
   ],
   "source": [
    "# use function to load image\n",
    "style_image = image_load(\"style1.jpg\")\n",
    "\n",
    "# let's print the shape to make sure it is what we expect\n",
    "# should be [1, 3, x, x] - batch dim, color channels, h x w\n",
    "print(style_image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also need to have a function that converts the tensors output by our model to images which can be displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert the tensors back to images\n",
    "# so that the image can be displayed\n",
    "\n",
    "def image_convert(tensor):\n",
    "    # clone the image\n",
    "    image = tensor.to(\"cpu\").clone().detach()\n",
    "    image = image.numpy().squeeze()\n",
    "    # convert from a tensor into a numpy array\n",
    "    image = image.transpose(1, 2, 0)\n",
    "    # undo normalization\n",
    "    image = image * np.array((0.229, 0.224, 0.225)) + np.array(\n",
    "        (0.485, 0.456, 0.406))\n",
    "    # clip the values to between 0 and 1\n",
    "    image = image.clip(0, 1)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to create functions that will calculate the loss for the content and style. We can calculate the content loss by getting the features of the content image. We'll have a function select the layers which handle the processing of the image features, the convolutional layers, and return the features. This same process can be applied to getting the Style Loss as well, although one extra function is needed to fully compute the style loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features(image, model, layers=None):\n",
    "    # if no layers are specified, use these layers\n",
    "    if layers is None:\n",
    "        layers = {'0': 'conv1_1', '5': 'conv2_1',\n",
    "                  '10': 'conv3_1',\n",
    "                  '19': 'conv4_1',\n",
    "                  '21': 'conv4_2',  ## content layer\n",
    "                  '28': 'conv5_1'}\n",
    "\n",
    "    # Dict to store the features\n",
    "    features = {}\n",
    "\n",
    "    x = image\n",
    "\n",
    "    # store the feature map responses if the name of the layer matches\n",
    "    # one of the keys in a given predefined layer dict.\n",
    "\n",
    "    for name, layer in enumerate(model.features):\n",
    "        # set x to the layer the image is passing through\n",
    "        x = layer(x)\n",
    "        # if the name of the layer is in the chosen layers, get the features from that layer\n",
    "        if str(name) in layers:\n",
    "            features[layers[str(name)]] = x\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also need to set up another function to finish getting the style loss. Computing the style loss is easily done by constructing a gram matrix. A gram matrix results from multiplying a matrix by its transpose. Given a matrix, it will contain feature maps FXL of layer L. FXL will be reshaped into F^XL, which is a KxN matrix where K is the number of feature maps at layer L and N is the length of any vectorized feature map FkXL.\n",
    "\n",
    "The gram matrix needs to be normalized. Normalizing the gram matrix can be done by dividing every element in the matrix by the total number of elements. If this is not done, large dimension N values can negatively impact gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(tensor):\n",
    "    # get the number of filters, height and width (channel doesn't matter)\n",
    "    _, num_filters, h, w = tensor.size()\n",
    "    tensor = tensor.view(num_filters, h * w)\n",
    "    # matrix multiplication against the transpose to get the gram matrix\n",
    "    gram = torch.mm(tensor, tensor.t())\n",
    "\n",
    "    return gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to select a model to carry out the transfer with. Our convolutional neural network in this case will be VGG19. The VGG19 model will take in predfined weights, which we can load in from PyTorch's website. We won't be computing any gradients for the predefined model, so we need to be sure that `requires_grad` is set to false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the VGG model and set requires grad to False\n",
    "\n",
    "torch.utils.model_zoo.load_url('https://download.pytorch.org/models/vgg19-dcbb9e9d.pth', model_dir='/home/daniel/Downloads/saved_weights/')\n",
    "\n",
    "vgg_model = models.vgg19()\n",
    "vgg_model.load_state_dict(torch.load('/home/daniel/Downloads/saved_weights/vgg19-dcbb9e9d.pth'))\n",
    "\n",
    "for param in vgg_model.parameters():\n",
    "    param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this isn't necessary, we can replace the Max Pooling layers with Average Pooling layers, as the Average Pooling layers tend to perform a little better for style transfer tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace max pool with average pooling layers, as they seem to perform better\n",
    "for i, layer in enumerate(vgg_model.features):\n",
    "    # if it matches a max pooling layer, replace\n",
    "    if isinstance(layer, torch.nn.MaxPool2d):\n",
    "        vgg_model.features[i] = torch.nn.AvgPool2d(kernel_size=2, stride=2, padding=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can declare the device we are using, the CUDA if it is available, and send the model to the device in evaluation mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace)\n",
       "    (4): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace)\n",
       "    (9): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace)\n",
       "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU(inplace)\n",
       "    (18): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace)\n",
       "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (24): ReLU(inplace)\n",
       "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (26): ReLU(inplace)\n",
       "    (27): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace)\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (31): ReLU(inplace)\n",
       "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (33): ReLU(inplace)\n",
       "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (35): ReLU(inplace)\n",
       "    (36): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Dropout(p=0.5)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace)\n",
       "    (5): Dropout(p=0.5)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vgg_model.to(device).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to load in the content and style images and send them to the device as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send both images to the device\n",
    "content = image_load(\"content1.jpg\").to(device)\n",
    "style = style_image.to(device)\n",
    "\n",
    "# extract the feature maps from both of the images\n",
    "features_content = select_features(content, vgg_model)\n",
    "features_style = select_features(style, vgg_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the feature extraction function we defined earlier, we'll used the pretrained model to get the features for both the content and style images. We'll then get the gram matrix for the style features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the gram matrices for the style layers\n",
    "style_grams = {layer: gram_matrix(features_style[layer]) for layer in features_style}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have to create a third image that will be transformed into our target image tensor. We can either create a random image or copy the content image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an image to transform, make random image\n",
    "target_image = torch.randn_like(content).requires_grad_(True).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've selected the features for the style and content images, but since the network didn't compute weights we'll have to specify these ourselves. We need the weights to finally retrieve the loss for the content and style images. \n",
    "\n",
    "We'll select mutiple convolutional layers and give them weights, this is because the different layers define different portions of the style, or contribute in different ways to its representation. Since there are different style layers, we can define the style weights individually. We'll use a multiplicative weight scheme for the different layers\n",
    "meaning we can edit these values and tune the style artifacts to our liking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the style weights individually\n",
    "style_image_weights = {'conv1_1': 0.75,\n",
    "                 'conv2_1': 0.5,\n",
    "                 'conv3_1': 0.2,\n",
    "                 'conv4_1': 0.2,\n",
    "                 'conv5_1': 0.2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need something that tracks the weight of the individual loss terms for content and style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# track individual loss terms for content and style\n",
    "content_weight = 1e4\n",
    "style_weight = 1e2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're almost ready to start the training process. In terms of handling the loss for the content, it is just MSE Loss between the feature map responses of both the content image and target image. Meanwhile, the style loss is similar but the feature maps are replaced by the divided gram matricies while the MSE loss is divided by the total number of elements in the respective feature map. Before we create the training loop, we'll choose an optimizer to use. The Adam optimizer should work fine in this instance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare the optimizer we're going to use\n",
    "\n",
    "optimizer = optim.Adam([target_image], lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we decide how many iterations we want to run the training cycle for, we'll crate the transformation loop. The loop computes the losses for style and content, multiplies the losses by the weights, and then sums them together to get the total loss.\n",
    "\n",
    "After the total loss is calculated, backpropogation is done and the valuee of the pixels are updated until the iterations are done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Iteration: 50, Total loss: 40195.97 - (content: 1400.77, style 1400.77)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-01f4dca95bd5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontent_loss\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstyle_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;31m# do backprop and optimize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mtotal_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\daniel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \"\"\"\n\u001b[1;32m--> 107\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\daniel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(1, 6000 + 1):\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # get the features from the target image and model\n",
    "    target_features = select_features(target_image, vgg_model)\n",
    "\n",
    "    # compute contnet loss\n",
    "    content_loss = torch.mean((target_features['conv4_2'] - features_content['conv4_2']) ** 2)\n",
    "\n",
    "    # set inital style loss to zero\n",
    "    style_loss = 0\n",
    "\n",
    "    # get the individual target features\n",
    "    for layer in style_image_weights:\n",
    "        target_feature = target_features[layer]\n",
    "        # compute the gram matrix\n",
    "        target_gram = gram_matrix(target_feature)\n",
    "        # find the shape of the target feature\n",
    "        _, d, h, w = target_feature.shape\n",
    "        # get the style gram of the current layer\n",
    "        style_gram = style_grams[layer]\n",
    "        # compute the loss for the current style layer\n",
    "        layer_style_loss = style_image_weights[layer] * torch.mean((target_gram - style_gram) ** 2)\n",
    "        # update the total style loss\n",
    "        style_loss += layer_style_loss / (d * h * w)\n",
    "\n",
    "    content_loss = content_weight * content_loss\n",
    "    style_loss = style_weight * style_loss\n",
    "    total_loss = content_loss + style_loss\n",
    "    # do backprop and optimize\n",
    "    total_loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "\n",
    "    # every 50 iterations, print out statistics\n",
    "    if i % 50 == 0:\n",
    "        total_loss_rounded = round(total_loss.item(), 2)\n",
    "        # proportion of loss belonging to content\n",
    "        content_fraction = round(content_weight*content_loss.item()/total_loss.item(), 2)\n",
    "        # proportion of loss belonging to style\n",
    "        style_fraction = round(content_weight*content_loss.item()/total_loss.item(), 2)\n",
    "        # Print the current iteration and both the content and style loss\n",
    "        print('Current Iteration: {}, Total loss: {} - (content: {}, style {})'.format(i, total_loss_rounded, content_fraction, style_fraction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the training is finished, the final image can be saved to a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can carry out our training and create the final image\n",
    "created_img = image_convert(target_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualize the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's visualize the image\n",
    "fig = plt.figure()\n",
    "plt.imshow(created_img)\n",
    "plt.axis('off')\n",
    "plt.savefig('shinkawa-lastofus.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
